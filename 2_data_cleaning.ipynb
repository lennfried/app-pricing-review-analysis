{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import needed packages for cleaning-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read exported CSV file from scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop all values which are not english, drop all duplicates and keep most relevent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['lang'] == 'en']\n",
    "df = df[df['source'] == 'most_relevant']\n",
    "df = df.sort_values(['app_name','source'], ascending=True)\n",
    "df = df.drop_duplicates(subset=['reviewId'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['at'] = pd.to_datetime(df['at'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app_grouped_df = df.groupby(['app_name','source']).size().reset_index().to_csv('first_check.csv')\n",
    "    #comment: first check how many english reviews for each app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Keep 200 newest reviews for each app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('app_name').apply(lambda x: x.sort_values(['at'],ascending=False).head(200)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#app_grouped_df = df.groupby(['app_name','source']).size().reset_index().to_csv('second_check.csv')\n",
    "    #comment: second check how many english reviews for each app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('reviews_en_only.csv')\n",
    "    #comment: export whole data file to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_reviews_df = df[['app_name','at','content','score']].to_csv('small_reviews_en.csv')\n",
    "    #comment: reduce data frame to relevant columns: 'app_name', 'at', 'content' and 'score' and export as CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add package to package 'spacy' for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data cleaning for the comlumn 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_lowercase'] = df['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))     #transform column content into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):         #remove emojis from column content\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "df['content_emoji'] = df['content_lowercase'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space(comment):         #do lemmatization in column content\n",
    "    doc = nlp(comment)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "df['content_lemma']= df['content_emoji'].apply(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/77/4cq7qf2n74g46m3l170t5fwh0000gn/T/ipykernel_5042/1416899279.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['content_punct'] = df['content_lemma'].str.replace('[^\\w\\s]','') #remove punctuation from column content\n"
     ]
    }
   ],
   "source": [
    "df['content_punct'] = df['content_lemma'].str.replace('[^\\w\\s]','')     #remove punctuation from column content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_punct'] = df['content_punct'].apply(lambda x: \" \".join(x.lower() for x in x.split()))       #transform column content into lower case a second time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "aditional_stopwords = [\"app\", \"food\", \"use\", \"weight\", \"track\", \"calorie\", \"diet\", \"fitness\", \"get\", \"make\", \"really\", \"keep\", \"try\", \"even\", \"meal\"]       #list of aditional stopwords\n",
    "stop = stopwords.words('english')          \n",
    "stop.extend(aditional_stopwords)\n",
    "df['content_nonstop'] = df['content_punct'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))        #remove stopwords from column content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Export full and smaller data file as CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('full_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_reviews_clean_df = df[['app_name','at','content', 'content_nonstop', 'score']].to_csv('small_data_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ea4b1123946223b3ed1d44f9daa52845c69c43204c1bbbcdd51b1001255e945"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
